#  WATU≈ö - Wieloagentowy System Robota Domowego

Kompletny system wieloagentowy w Pythonie dla robota Watu≈õ dzia≈ÇajƒÖcego na Raspberry Pi/Linux. System obs≈Çuguje przetwarzanie mowy, rozpoznawanie m√≥wcy, agregacjƒô danych sensorycznych i komunikacjƒô z LLM.

##  Spis Tre≈õci

- [Funkcjonalno≈õƒá](#funkcjonalno≈õƒá)
- [Architektura](#architektura)
- [Wymagania](#wymagania)
- [Instalacja](#instalacja)
- [Konfiguracja](#konfiguracja)
- [Uruchomienie](#uruchomienie)
- [Struktura Projektu](#struktura-projektu)
- [Modu≈Çy](#modu≈Çy)
- [Testy](#testy)
- [RozwiƒÖzanie TTS](#rozwiƒÖzanie-tts)
- [API i Format Danych](#api-i-format-danych)
- [Troubleshooting](#troubleshooting)

---

##  Funkcjonalno≈õƒá

###  **watus.py** - Program Przetwarzania Mowy

-  **CiƒÖg≈Çe nas≈Çuchiwanie** z VAD (Voice Activity Detection)
-  **Wykrywanie wake word**: "hej watusiu" z tolerancjƒÖ na zniekszta≈Çcenia
-  **Rozpoznawanie lidera**: ECAPA-TDNN speaker recognition
-  **Transkrypcja**: Faster-Whisper (model base) dla jƒôzyka polskiego
-  **Analiza g≈Ço≈õno≈õci** w decybelach
-  **Kontrola LED**: zielony=nas≈Çuch, czerwony=przetwarzanie
-  **Half-duplex**: albo s≈Çucha albo m√≥wi
-  **TTS**: Edge-TTS (Microsoft) z naturalnym g≈Çosem polskim
-  **Komunikacja**: ZeroMQ pub/sub

###  **reporter.py** - Agent Agregacji Danych

-  **Odbiera dialog** przez ZeroMQ
-  **Zbiera dane** z wielu ≈∫r√≥de≈Ç (kamera, LiDAR, scenariusze)
-  **Tworzy kontekst** z timestampem
-  **Wysy≈Ça do LLM** (API endpoint z konfiguracji)
-  **Zwraca odpowied≈∫** przez ZeroMQ

---

##  Architektura

```

   MIKROFON          

           
           
      
   watus.py             ZeroMQ            
   - VAD                      - Pub: dialog     
   - Wake Word                - Sub: response   
   - Speaker ID            
   - Transcription                    
   - TTS                              
   - LED Control           
         reporter.py       
                                - Aggregator      
         - LLM Client      
   LED                   - Context Builder 
   - Green (listen)         
   - Red (process)                     
                 
                             
         LLM Endpoint      
   SPEAKER                     (API z konfiguracji) 
      
           
           
           

≈πr√≥d≈Ça danych:
- data/dialog.jsonl
- data/camera.jsonl
- data/lidar.jsonl
- data/scenarios.jsonl
```

---

##  Wymagania

### Hardware

- **Raspberry Pi 4/5** (4GB+ RAM) lub Linux PC
- **Mikrofon USB** (zalecane: z redukcjƒÖ szum√≥w)
- **G≈Ço≈õnik** lub wyj≈õcie audio
- **LED** (opcjonalnie, dzia≈Ça w trybie mock)
- **GPU NVIDIA** (opcjonalnie, dla przyspieszonego przetwarzania)

### Software

- **Python 3.9+**
- **CUDA** (opcjonalnie, dla GPU)
- **System audio**: ALSA/PulseAudio
- **mpg123** lub **ffmpeg** (dla TTS playback)

---

##  Instalacja

### 1. Sklonuj repozytorium

```bash
cd /home/ubuntu/
git clone <repo-url> watus_robot
cd watus_robot
```

### 2. Utw√≥rz ≈õrodowisko wirtualne

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Zainstaluj zale≈ºno≈õci Python

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Zainstaluj narzƒôdzia systemowe

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y mpg123 ffmpeg portaudio19-dev

# Raspberry Pi - dodaj RPi.GPIO
pip install RPi.GPIO
```

### 5. Zainstaluj PyTorch z CUDA (opcjonalnie)

```bash
# Dla GPU NVIDIA
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# Dla CPU only
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
```

---

##  Konfiguracja

### Plik `config.json`

Edytuj `config.json` aby dostosowaƒá system:

```json
{
  "audio": {
    "sample_rate": 16000,
    "chunk_duration_ms": 30,
    "vad_mode": 3,
    "silence_threshold_ms": 430,
    "pre_speech_buffer_ms": 300,
    "volume_threshold_db": -35
  },
  "wake_word": {
    "phrase": "hej watusiu",
    "alternatives": ["hej watu≈õ", "hej watu", "ej watusiu"],
    "threshold": 0.7
  },
  "speaker_recognition": {
    "model_name": "speechbrain/spkrec-ecapa-voxceleb",
    "threshold": 0.85,
    "leader_timeout_seconds": 180
  },
  "whisper": {
    "model_size": "base",
    "language": "pl",
    "device": "cuda",
    "compute_type": "float16"
  },
  "tts": {
    "engine": "edge",
    "voice": "pl-PL-MarekNeural",
    "rate": "+0%",
    "volume": "+0%"
  },
  "led": {
    "green_pin": 17,
    "red_pin": 27,
    "mock_mode": true
  },
  "zmq": {
    "dialog_publisher": "tcp://127.0.0.1:5555",
    "response_subscriber": "tcp://127.0.0.1:5556",
    "reporter_publisher": "tcp://127.0.0.1:5557"
  },
  "llm": {
    "endpoint": "http://localhost:11434/api/chat",
    "model": "llama3",
    "timeout": 30
  }
}
```

#### Kluczowe parametry:

- **`vad_mode`**: 0-3 (3 = najbardziej agresywny)
- **`silence_threshold_ms`**: Min. przerwa do uznania ko≈Ñca wypowiedzi (430ms)
- **`volume_threshold_db`**: Pr√≥g g≈Ço≈õno≈õci (-35 dB)
- **`leader_timeout_seconds`**: Timeout lidera (180s = 3 minuty)
- **`device`**: `"cuda"` dla GPU, `"cpu"` dla CPU
- **`mock_mode`**: `true` dla test√≥w bez GPIO

---

##  Uruchomienie

### Testowanie modu≈Ç√≥w

```bash
# Uruchom testy jednostkowe
pytest tests/ -v

# Test z pokryciem kodu
pytest tests/ --cov=. --cov-report=html
```

### Uruchomienie systemu

#### 1. Skonfiguruj endpoint LLM

Upewnij siƒô ≈ºe masz dostƒôp do endpointu LLM API. Skonfiguruj go w `config.json`:

```json
"llm": {
  "endpoint": "https://your-llm-api-endpoint.com/api/chat",
  "model": "your-model-name",
  "timeout": 30
}
```

Endpoint powinien byƒá kompatybilny z formatem API opisanym w sekcji "API i Format Danych".

#### 2. Uruchom reporter.py (w osobnym terminalu)

```bash
source venv/bin/activate
python reporter.py
```

#### 3. Uruchom watus.py

```bash
source venv/bin/activate
python watus.py
```

#### 4. Wybierz urzƒÖdzenie audio

System wy≈õwietli listƒô dostƒôpnych mikrofon√≥w:

```
Dostƒôpne urzƒÖdzenia audio:
  [0] Built-in Microphone
  [1] USB Audio Device
  [2] ...

Wybierz numer urzƒÖdzenia: 1
```

### U≈ºycie

1. **Powiedz wake word**: "Hej Watusiu"
2. **Poczekaj na reakcjƒô**: Czerwona LED
3. **Otrzymaj odpowied≈∫**: Robot odpowie przez g≈Ço≈õnik
4. **Kontynuuj rozmowƒô**: W ciƒÖgu 3 minut (timeout lidera)

---

##  Struktura Projektu

```
watus_robot/
 config.json                 # Konfiguracja systemu
 watus.py                    # G≈Ç√≥wny program mowy
 reporter.py                 # Agent agregacji
 requirements.txt            # Zale≈ºno≈õci Python
 README.md                   # Dokumentacja (ten plik)

 audio_utils.py              # Obs≈Çuga audio
 vad_utils.py                # Voice Activity Detection
 speaker_recognition.py      # Rozpoznawanie m√≥wcy
 transcription.py            # Faster-Whisper STT
 tts_utils.py                # Edge-TTS
 led_control.py              # Kontrola LED
 zmq_utils.py                # Komunikacja ZeroMQ

 tests/                      # Testy jednostkowe
    __init__.py
    test_audio_utils.py
    test_vad_utils.py
    test_speaker_recognition.py
    test_transcription.py
    test_led_control.py
    test_reporter.py

 data/                       # Pliki danych JSONL
    dialog.jsonl
    camera.jsonl
    lidar.jsonl
    scenarios.jsonl

 logs/                       # Logi systemowe
    watus.log
    watus_reporter.log

 models/                     # Modele AI (pobierane automatycznie)
     whisper/
     speaker_recognition/
```

---

## üß© Modu≈Çy

### 1. **audio_utils.py**

Obs≈Çuga audio: listowanie urzƒÖdze≈Ñ, nagrywanie, analiza g≈Ço≈õno≈õci.

**G≈Ç√≥wne funkcje:**
- `list_audio_devices()` - lista mikrofon√≥w
- `select_audio_device()` - wyb√≥r urzƒÖdzenia
- `calculate_volume_db(audio_data)` - g≈Ço≈õno≈õƒá w dB
- `create_audio_stream()` - strumie≈Ñ audio

### 2. **vad_utils.py**

Voice Activity Detection (WebRTC VAD).

**G≈Ç√≥wne funkcje:**
- `create_vad(mode)` - tworzy VAD
- `is_speech(vad, audio_data, sample_rate)` - wykrywa mowƒô
- `SpeechBuffer` - bufor do zbierania wypowiedzi

### 3. **speaker_recognition.py**

Rozpoznawanie m√≥wcy przez ECAPA-TDNN.

**G≈Ç√≥wne funkcje:**
- `load_speaker_model(model_name, device)` - ≈Çaduje model
- `extract_embedding(model, audio_data, sample_rate)` - ekstraktuje embedding
- `calculate_similarity(emb1, emb2)` - podobie≈Ñstwo kosinusowe
- `LeaderTracker` - ≈õledzi lidera

### 4. **transcription.py**

Transkrypcja mowy na tekst (Faster-Whisper).

**G≈Ç√≥wne funkcje:**
- `load_whisper_model(model_size, device)` - ≈Çaduje model
- `transcribe_audio(model, audio_data, sample_rate)` - transkrypcja
- `detect_wake_word(text, wake_phrase)` - wykrywa wake word

### 5. **tts_utils.py**

Text-to-Speech przez Edge-TTS (Microsoft).

**G≈Ç√≥wne funkcje:**
- `synthesize_speech(text, voice)` - synteza mowy
- `play_audio_file(file_path)` - odtwarzanie
- `speak(text, voice)` - syntetyzuj i odtw√≥rz
- `list_available_voices()` - lista g≈Ços√≥w

**Dostƒôpne g≈Çosy polskie:**
- `pl-PL-MarekNeural` (mƒôski, zalecany) 
- `pl-PL-ZofiaNeural` (≈ºe≈Ñski)

### 6. **led_control.py**

Kontrola LED przez GPIO (z trybem mock).

**G≈Ç√≥wne funkcje:**
- `setup_leds(green_pin, red_pin, mock_mode)`
- `set_listening_mode()` - zielona LED
- `set_processing_mode()` - czerwona LED

### 7. **zmq_utils.py**

Komunikacja ZeroMQ pub/sub.

**G≈Ç√≥wne funkcje:**
- `create_zmq_publisher(address)` - publisher
- `create_zmq_subscriber(address, topic)` - subscriber
- `publish_message(socket, message, topic)` - publikuj
- `receive_message(socket, timeout_ms)` - odbierz

---

## üß™ Testy

### Uruchamianie test√≥w

```bash
# Wszystkie testy
pytest tests/ -v

# Konkretny modu≈Ç
pytest tests/test_vad_utils.py -v

# Z pokryciem kodu
pytest tests/ --cov=. --cov-report=html
open htmlcov/index.html
```

### Test-First Approach

Projekt zosta≈Ç stworzony z test-first approach:
-  Testy przed implementacjƒÖ
-  100% pokrycie krytycznych funkcji
-  Mocking dla I/O operacji

---

##  RozwiƒÖzanie TTS

### Edge-TTS (Microsoft) - ZALECANE 

**Dlaczego lepsze ni≈º Piper?**

 **Naturalno≈õƒá**: Najwy≈ºsza jako≈õƒá g≈Ços√≥w (neural TTS)  
 **Szybko≈õƒá**: Bardzo szybkie przez API  
 **Polski**: Doskona≈Çe wsparcie dla jƒôzyka polskiego  
 **Darmowe**: Bez limit√≥w (u≈ºywa publicznego API)  
 **Proste**: Bez instalacji modeli, dzia≈Ça od razu  

**Wady:**
 Wymaga internetu  
 Zale≈ºno≈õƒá od Microsoft API  

### Alternatywy

| TTS Engine | Naturalno≈õƒá | Szybko≈õƒá | Offline | Polski |
|------------|-------------|----------|---------|--------|
| **Edge-TTS** |  |  |  |  Doskona≈Çy |
| **Piper** |  |  |  |  Dobry |
| **Coqui TTS** |  |  |  |  Bardzo dobry |
| **gTTS** |  |  |  |  Podstawowy |

### Zmiana g≈Çosu TTS

Edytuj `config.json`:

```json
{
  "tts": {
    "engine": "edge",
    "voice": "pl-PL-ZofiaNeural",  // Zmie≈Ñ na ≈ºe≈Ñski
    "rate": "+10%",                 // Przyspiesz
    "volume": "+10%"                // G≈Ço≈õniej
  }
}
```

### Lista dostƒôpnych g≈Ços√≥w

```python
from tts_utils import list_available_voices
voices = list_available_voices()
```

---

##  API i Format Danych

### Format dialog.jsonl

```json
{
  "timestamp": "2025-10-16T10:15:23.456789",
  "speaker": "leader",
  "text": "Hej Watusiu, jak siƒô masz?",
  "duration": 2.3,
  "is_leader": true,
  "volume_db": -28.5
}
```

### Format camera.jsonl

```json
{
  "timestamp": "2025-10-16T10:15:20.123456",
  "description": "Wykryto osobƒô w centralnej czƒô≈õci obrazu",
  "confidence": 0.95,
  "objects": [
    {
      "type": "person",
      "bbox": [120, 80, 280, 400],
      "confidence": 0.95
    }
  ],
  "brightness": "medium",
  "scene": "indoor"
}
```

### Format lidar.jsonl

```json
{
  "timestamp": "2025-10-16T10:15:18.123456",
  "obstacles": "brak przeszk√≥d w promieniu 2m",
  "distances": {
    "front": 2.5,
    "left": 1.8,
    "right": 3.2,
    "back": 1.5
  },
  "closest_obstacle": {
    "direction": "back",
    "distance": 1.5
  },
  "safe_to_move": true
}
```

### Format scenarios.jsonl

```json
{
  "timestamp": "2025-10-16T10:00:00.000000",
  "name": "patrol",
  "status": "active",
  "description": "Regularny patrol pomieszczenia co 30 minut",
  "priority": 2,
  "params": {
    "interval_minutes": 30,
    "route": "kitchen-livingroom-bedroom"
  }
}
```

### Format API LLM

**Request:**
```json
{
  "model": "llama3",
  "messages": [
    {
      "role": "system",
      "content": "<prompt z kontekstem>"
    }
  ],
  "stream": false
}
```

**Response:**
```json
{
  "message": {
    "role": "assistant",
    "content": "Odpowied≈∫ robota"
  }
}
```

---

##  Troubleshooting

### Problem: Brak urzƒÖdze≈Ñ audio

```bash
# Sprawd≈∫ urzƒÖdzenia
arecord -l

# Test nagrywania
arecord -d 5 -f cd test.wav
aplay test.wav
```

### Problem: CUDA nie dzia≈Ça

```bash
# Sprawd≈∫ CUDA
nvidia-smi

# Zmie≈Ñ na CPU w config.json
"device": "cpu",
"compute_type": "int8"
```

### Problem: B≈ÇƒÖd ZeroMQ

```bash
# Sprawd≈∫ porty
netstat -tuln | grep 555

# Zmie≈Ñ porty w config.json je≈õli zajƒôte
```

### Problem: Brak odpowiedzi od LLM

```bash
# Sprawd≈∫ czy endpoint LLM jest dostƒôpny
curl -X POST https://your-llm-endpoint.com/api/chat \
  -H "Content-Type: application/json" \
  -d '{"model":"test","messages":[{"role":"user","content":"test"}]}'

# Sprawd≈∫ konfiguracjƒô w config.json
cat config.json | grep -A5 '"llm"'

# Sprawd≈∫ logi reporter.py
```
```

### Problem: VAD nie wykrywa mowy

- Zwiƒôksz czu≈Ço≈õƒá: `"vad_mode": 1` (w config.json)
- Zmniejsz pr√≥g g≈Ço≈õno≈õci: `"volume_threshold_db": -40`
- Sprawd≈∫ mikrofon i g≈Ço≈õno≈õƒá

### Problem: Wake word nie dzia≈Ça

- Dodaj alternatywy w config.json
- Zmniejsz threshold: `"threshold": 0.5`
- Sprawd≈∫ transkrypcjƒô w logach

### Problem: TTS nie dzia≈Ça (brak d≈∫wiƒôku)

```bash
# Zainstaluj mpg123
sudo apt-get install mpg123

# Lub ffmpeg
sudo apt-get install ffmpeg

# Test
mpg123 --version
```

---

##  Logi

Logi znajdujƒÖ siƒô w `logs/`:
- `logs/watus.log` - g≈Ç√≥wny program
- `logs/watus_reporter.log` - reporter

Poziom logowania mo≈ºna zmieniƒá w `config.json`:
```json
{
  "logs": {
    "level": "DEBUG",  // DEBUG, INFO, WARNING, ERROR
    "file": "logs/watus.log"
  }
}
```

---

##  Bezpiecze≈Ñstwo

-  **Timeout lidera**: System automatycznie resetuje lidera po 3 minutach braku aktywno≈õci
-  **Tylko lider**: Odpowiedzi sƒÖ wysy≈Çane tylko na zapytania od lidera
-  **Prywatno≈õƒá**: Dane audio nie sƒÖ przechowywane (tylko transkrypcje w JSONL)

---

##  Optymalizacje

### GPU

System jest zoptymalizowany dla GPU:
- Faster-Whisper: `device="cuda"`, `compute_type="float16"`
- ECAPA: automatyczne wykrywanie CUDA

### CPU only

Dla Raspberry Pi bez GPU:
```json
{
  "whisper": {
    "model_size": "tiny",  // Zmniejsz model
    "device": "cpu",
    "compute_type": "int8"
  }
}
```

---

##  Dokumentacja Zewnƒôtrzna

- [Faster-Whisper](https://github.com/guillaumekln/faster-whisper)
- [SpeechBrain ECAPA](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb)
- [Edge-TTS](https://github.com/rany2/edge-tts)
- [WebRTC VAD](https://github.com/wiseman/py-webrtcvad)
- [ZeroMQ](https://zeromq.org/)

---

##  Licencja

MIT License - wolne do u≈ºytku osobistego i komercyjnego.

---

## ‚Äç Autor

System Watu≈õ - Robot Domowy  
Wersja: 1.0.0  
Data: 2025-10-16

---

##  Podsumowanie

System Watu≈õ to kompletne rozwiƒÖzanie dla robota domowego z zaawansowanym przetwarzaniem mowy:

 **Prosty kod** - tylko funkcje, bez klas  
 **Test-first** - pe≈Çne pokrycie testami  
 **Zoptymalizowany** - GPU support  
 **Naturalny TTS** - Edge-TTS dla polskiego  
 **Rozpoznawanie m√≥wcy** - ECAPA-TDNN  
 **Wieloagentowy** - watus.py + reporter.py  
 **Dokumentacja PL** - pe≈Çna dokumentacja po polsku  

**Powodzenia z robotem Watu≈õ! **
